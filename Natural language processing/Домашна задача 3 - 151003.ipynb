{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашна задача 3\n",
    "## Обработка на природните јазици 2018/2019\n",
    "### Андреј Јанчевски - 151003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T02:26:39.976520Z",
     "start_time": "2018-12-19T02:26:39.305521Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sklearn\n",
    "import pandas\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преземање на податочното множество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T20:52:54.327817Z",
     "start_time": "2018-12-06T20:52:53.835808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\bani5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bani5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bani5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bani5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T20:52:55.140884Z",
     "start_time": "2018-12-06T20:52:54.491005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['one', 'of', 'the', '90s', \"'\", 'most', 'unwelcome', ...], 'neg'),\n",
       " (['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], 'neg'),\n",
       " (['allen', ',', 'star', 'of', 'many', 'a', 'brian', ...], 'pos'),\n",
       " (['national', 'lampoon', \"'\", 's', 'animal', 'house', ...], 'pos'),\n",
       " (['blade', 'is', 'the', 'movie', 'that', 'shows', ...], 'pos'),\n",
       " (['a', 'cop', 'with', 'a', 'troubled', 'personal', ...], 'neg'),\n",
       " (['warning', ':', 'this', 'review', 'contains', 'some', ...], 'pos'),\n",
       " (['the', 'american', 'action', 'film', 'has', 'been', ...], 'pos'),\n",
       " (['good', 'films', 'are', 'hard', 'to', 'find', 'these', ...], 'pos'),\n",
       " (['after', 'the', 'recent', 'animated', 'debacles', ...], 'neg'),\n",
       " (['working', 'in', 'the', 'motion', 'picture', ...], 'neg'),\n",
       " (['the', 'tagline', 'for', 'this', 'film', 'is', ':', ...], 'neg'),\n",
       " (['what', 'were', 'they', 'thinking', '?', 'nostalgia', ...], 'neg'),\n",
       " (['surrounded', 'by', 'hype', ',', 'high', 'hopes', ',', ...], 'neg'),\n",
       " (['bob', 'the', 'happy', 'bastard', \"'\", 's', 'quickie', ...], 'pos'),\n",
       " (['ingredients', ':', 'neophyte', 'lawyer', ',', ...], 'pos'),\n",
       " (['david', 'cronenberg', 'presents', 'us', 'with', ...], 'pos'),\n",
       " (['`', 'oh', 'behave', '!', 'felicity', 'shagwell', ...], 'pos'),\n",
       " (['what', 'would', 'you', 'do', 'if', 'no', 'one', ...], 'neg'),\n",
       " (['the', 'full', 'monty', 'is', 'a', 'whole', 'lot', ...], 'pos'),\n",
       " (['are', 'you', 'tired', 'of', 'all', 'the', 'hot', ...], 'pos'),\n",
       " (['the', 'first', 'scene', 'of', 'operation', 'condor', ...], 'neg'),\n",
       " (['the', 'makers', 'of', 'spawn', 'have', 'created', ...], 'neg'),\n",
       " (['yet', 'another', 'brainless', 'teen', 'flick', ',', ...], 'neg'),\n",
       " (['there', \"'\", 's', 'a', '1', ',', '000', '-', 'foot', ...], 'neg'),\n",
       " (['stephen', ',', 'please', 'post', 'if', 'appropriate', ...], 'neg'),\n",
       " (['an', 'unhappy', 'italian', 'housewife', ',', 'a', ...], 'pos'),\n",
       " (['hedwig', '(', 'john', 'cameron', 'mitchell', ')', ...], 'pos'),\n",
       " (['i', 'recall', 'the', 'trials', 'and', 'tribulations', ...], 'pos'),\n",
       " (['charlie', 'sheen', 'stars', 'as', 'zane', ',', 'a', ...], 'pos'),\n",
       " (['warning', '!', ':', 'may', 'contain', 'some', 'mild', ...], 'pos'),\n",
       " (['america', \"'\", 's', 'favorite', 'homicidal', ...], 'neg'),\n",
       " (['melvin', 'udall', 'is', 'a', 'heartless', 'man', '.', ...], 'pos'),\n",
       " (['the', 'latest', 'epos', 'from', 'lars', 'is', 'a', ...], 'pos'),\n",
       " (['adam', 'sandler', 'isn', \"'\", 't', 'known', 'for', ...], 'neg'),\n",
       " (['to', 'put', 'it', 'bluntly', ',', 'ed', 'wood', ...], 'neg'),\n",
       " (['the', 'first', 'image', 'in', '\"', 'final', ...], 'neg'),\n",
       " (['so', 'much', 'for', 'sweet', 'returns', '.', 'after', ...], 'neg'),\n",
       " (['\"', 'i', 'seem', 'to', 'have', 'glued', 'myself', ...], 'neg'),\n",
       " (['capsule', ':', 'john', 'the', 'baptist', 'is', ...], 'neg'),\n",
       " (['synopsis', ':', 'retiring', 'detective', 'jerry', ...], 'pos'),\n",
       " (['you', 'don', \"'\", 't', 'have', 'to', 'know', 'poker', ...], 'pos'),\n",
       " (['it', \"'\", 's', 'always', 'a', 'bad', 'sign', 'when', ...], 'neg'),\n",
       " (['note', ':', 'some', 'may', 'consider', 'portions', ...], 'pos'),\n",
       " (['the', 'last', 'of', 'vampire', '-', 'films', ...], 'neg'),\n",
       " (['linda', 'fiorentino', 'disappeared', 'off', 'the', ...], 'pos'),\n",
       " (['i', 'have', 'to', 'say', 'it', '.', 'tim', 'burton', ...], 'pos'),\n",
       " (['for', 'this', 'review', 'and', 'more', ',', 'visit', ...], 'pos'),\n",
       " (['\"', 'jaws', '\"', 'is', 'a', 'rare', 'film', 'that', ...], 'pos'),\n",
       " (['seen', 'february', '15', ',', '1998', 'on', 'home', ...], 'pos'),\n",
       " (['after', 'indecent', 'proposal', 'and', 'up', 'close', ...], 'pos'),\n",
       " (['mpaa', ':', 'not', 'rated', '(', 'though', 'i', ...], 'pos'),\n",
       " (['it', \"'\", 's', 'almost', 'amusing', 'to', 'watch', ...], 'neg'),\n",
       " (['the', 'police', 'negotiator', 'is', 'the', 'person', ...], 'pos'),\n",
       " (['it', \"'\", 's', 'tough', 'to', 'be', 'an', 'aspiring', ...], 'neg'),\n",
       " (['as', 'the', 'film', 'opens', 'up', ',', 'expectant', ...], 'pos'),\n",
       " (['\"', 'when', 'it', \"'\", 's', 'cold', ',', 'molecules', ...], 'pos'),\n",
       " (['there', 'is', 'a', 'scene', 'in', 'patch', 'adams', ...], 'neg'),\n",
       " (['at', 'first', 'glance', ',', 'i', 'thought', 'that', ...], 'neg'),\n",
       " (['plot', ':', 'based', 'on', 'the', 'wildly', ...], 'neg'),\n",
       " (['as', 'bad', 'as', '\"', 'mimic', '\"', 'was', ',', ...], 'neg'),\n",
       " (['written', 'by', 'alex', 'cox', ',', 'tod', 'davies', ...], 'neg'),\n",
       " (['i', \"'\", 'm', 'really', 'starting', 'to', 'wonder', ...], 'neg'),\n",
       " (['synopsis', ':', 'melissa', ',', 'a', 'mentally', '-', ...], 'neg'),\n",
       " (['dora', '(', 'fernanda', 'montenegro', ')', 'sits', ...], 'pos'),\n",
       " (['when', 'i', 'first', 'heard', 'that', 'romeo', '&', ...], 'pos'),\n",
       " (['buffalo', '?', '66', 'is', 'a', 'very', 'rarely', ...], 'pos'),\n",
       " (['wyatt', 'earp', 'has', 'a', 'lot', 'to', 'tell', ...], 'pos'),\n",
       " (['the', 'relaxed', 'dude', 'rides', 'a', 'roller', ...], 'pos'),\n",
       " (['ingredients', ':', 'man', 'with', 'amnesia', 'who', ...], 'pos'),\n",
       " (['i', 'had', 'been', 'expecting', 'more', 'of', 'this', ...], 'pos'),\n",
       " (['under', 'any', 'other', 'circumstances', ',', 'i', ...], 'neg'),\n",
       " (['if', 'the', 'current', 'trends', 'of', 'hollywood', ...], 'pos'),\n",
       " (['i', \"'\", 'll', 'bet', 'right', 'now', 'you', \"'\", ...], 'neg'),\n",
       " (['it', 'seemed', 'wholly', 'appropriate', 'that', 'at', ...], 'pos'),\n",
       " (['there', 'were', 'four', 'movies', 'that', 'earned', ...], 'neg'),\n",
       " (['james', 'jones', ',', 'one', 'of', 'the', 'major', ...], 'pos'),\n",
       " (['here', 'i', 'sit', 'at', 'my', 'computer', 'about', ...], 'neg'),\n",
       " (['`', 'strange', 'days', \"'\", 'chronicles', 'the', ...], 'pos'),\n",
       " (['i', 'rented', '\"', 'brokedown', 'palace', '\"', ...], 'pos'),\n",
       " (['it', 'seemed', 'like', 'the', 'perfect', 'concept', ...], 'neg'),\n",
       " (['yeah', ',', 'yeah', ',', 'the', 'advertisements', ...], 'neg'),\n",
       " (['remember', 'tom', 'cruise', 'and', 'brian', 'brown', ...], 'neg'),\n",
       " (['in', 'life', ',', 'eddie', 'murphy', 'and', 'martin', ...], 'neg'),\n",
       " (['the', 'first', 'film', 'produced', 'by', 'adam', ...], 'neg'),\n",
       " (['capsule', ':', 'the', 'weakest', 'and', 'least', ...], 'neg'),\n",
       " (['in', 'the', 'james', 'bond', 'film', '\"', 'diamonds', ...], 'neg'),\n",
       " (['forgive', 'the', 'fevered', 'criticism', 'but', ...], 'neg'),\n",
       " (['in', 'the', 'line', 'of', 'duty', 'is', 'the', ...], 'neg'),\n",
       " (['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], 'neg'),\n",
       " (['edward', 'zwick', \"'\", 's', '\"', 'the', 'siege', '\"', ...], 'pos'),\n",
       " (['in', 'the', 'year', '2029', ',', 'captain', 'leo', ...], 'neg'),\n",
       " (['dreamworks', 'pictures', 'presents', 'a', 'jinks', ...], 'pos'),\n",
       " (['usually', 'when', 'a', 'blockbuster', 'comes', 'out', ...], 'pos'),\n",
       " (['note', ':', 'some', 'may', 'consider', 'portions', ...], 'pos'),\n",
       " (['i', 'came', 'to', 'an', 'epiphany', 'while', ...], 'neg'),\n",
       " (['jerry', 'springer', 'has', 'got', 'nothing', 'on', ...], 'pos'),\n",
       " (['not', 'since', 'oliver', 'stone', \"'\", 's', ...], 'pos'),\n",
       " (['\"', 'mission', 'to', 'mars', '\"', 'is', 'one', 'of', ...], 'neg'),\n",
       " (['the', 'general', \"'\", 's', 'daughter', 'will', ...], 'neg'),\n",
       " (['trailing', 'the', 'success', 'of', 'brit', 'humour', ...], 'pos'),\n",
       " (['in', '\"', 'magic', 'town', '\"', ',', 'jimmy', ...], 'pos'),\n",
       " (['when', '_star', 'wars_', 'came', 'out', 'some', ...], 'pos'),\n",
       " (['after', 'the', 'huge', 'success', 'of', '\"', 'the', ...], 'neg'),\n",
       " (['plot', ':', 'a', 'young', 'french', 'boy', 'sees', ...], 'neg'),\n",
       " (['because', 'no', 'one', 'demanded', 'it', ':', ...], 'neg'),\n",
       " (['the', 'rich', 'legacy', 'of', 'cinema', 'has', ...], 'neg'),\n",
       " (['\"', 'i', \"'\", 'd', 'rather', 'die', 'today', 'and', ...], 'pos'),\n",
       " (['us', 'critic', '-', 'type', 'people', 'are', ...], 'pos'),\n",
       " (['apparently', ',', 'when', 'crap', 'calls', ',', ...], 'neg'),\n",
       " (['salaries', 'of', 'hollywood', 'top', 'actors', 'are', ...], 'neg'),\n",
       " (['on', 'april', '12th', ',', '1912', ',', 'the', ...], 'pos'),\n",
       " (['\"', 'i', 'would', 'appreciate', 'it', 'if', 'you', ...], 'neg'),\n",
       " (['even', 'if', 'i', 'did', 'not', 'know', 'that', ...], 'pos'),\n",
       " (['plunkett', '&', 'macleane', 'is', 'a', 'period', ...], 'neg'),\n",
       " (['the', 'kids', 'in', 'the', 'hall', 'are', 'an', ...], 'neg'),\n",
       " (['all', 'great', 'things', 'come', 'to', 'an', 'end', ...], 'pos'),\n",
       " (['capsule', ':', 'the', 'best', 'place', 'to', 'start', ...], 'pos'),\n",
       " (['how', 'do', 'films', 'like', 'mouse', 'hunt', 'get', ...], 'neg'),\n",
       " (['summary', 'five', 'liberal', 'iowa', 'graduate', ...], 'pos'),\n",
       " (['long', 'time', 'buddies', 'and', 'neil', 'diamond', ...], 'neg'),\n",
       " (['in', '1977', ',', 'something', 'never', 'though', ...], 'pos'),\n",
       " (['\"', 'alcohol', 'and', 'drugs', '=', 'bad', '.', ...], 'neg'),\n",
       " (['synopsis', ':', 'wealthy', 'cuban', 'landowner', ...], 'neg'),\n",
       " (['not', 'since', 'attending', 'an', 'ingmar', ...], 'pos'),\n",
       " (['up', 'until', 'about', 'a', 'year', 'ago', ',', ...], 'pos'),\n",
       " (['sometimes', ',', 'when', 'i', 'decide', 'to', ...], 'neg'),\n",
       " (['>', 'from', 'the', 'man', 'who', 'presented', 'us', ...], 'pos'),\n",
       " (['`', 'run', 'lola', 'run', \"'\", ',', 'a', 'german', ...], 'pos'),\n",
       " (['in', 'my', 'opinion', ',', 'a', 'movie', 'reviewer', ...], 'neg'),\n",
       " (['say', ',', 'tell', 'me', 'if', 'you', \"'\", 've', ...], 'neg'),\n",
       " (['in', 'this', 'year', \"'\", 's', 'summer', 'movie', ...], 'neg'),\n",
       " (['adam', 'sandler', 'vehicles', 'are', 'never', ...], 'neg'),\n",
       " (['i', 'was', 'originally', 'going', 'to', 'give', ...], 'pos'),\n",
       " (['wild', 'things', 'is', 'a', 'way', 'to', 'steam', ...], 'neg'),\n",
       " (['by', 'phil', 'curtolo', 'mel', 'gibson', '(', ...], 'pos'),\n",
       " (['warning', ':', 'contains', 'what', 'the', 'matrix', ...], 'pos'),\n",
       " (['when', 'the', 'film', 'features', 'richard', 'lynch', ...], 'neg'),\n",
       " (['once', 'upon', 'a', 'time', 'jean', '-', 'claude', ...], 'neg'),\n",
       " (['i', 'was', 'pleasantly', 'surprised', 'by', 'this', ...], 'pos'),\n",
       " (['plot', ':', 'a', 'rich', 'guy', 'who', 'doesn', \"'\", ...], 'neg'),\n",
       " (['synopsis', ':', 'bobby', 'garfield', '(', 'yelchin', ...], 'pos'),\n",
       " (['fact', 'that', 'charles', 'bronson', 'represents', ...], 'neg'),\n",
       " (['based', 'on', 'the', '1960s', 'tv', 'series', 'that', ...], 'neg'),\n",
       " (['with', 'the', 'success', 'of', 'the', 'surprise', ...], 'pos'),\n",
       " (['mugshot', '(', 'director', '/', 'writer', '/', ...], 'neg'),\n",
       " (['"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 68239 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "categories = movie_reviews.categories()\n",
    "documents = []\n",
    "for category in categories:\n",
    "    for file_id in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(file_id), category))\n",
    "random.seed(57)\n",
    "random.shuffle(documents)\n",
    "num_documents=len(documents)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пресметување на карактеристиките за множеството"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T20:52:55.979556Z",
     "start_time": "2018-12-06T20:52:55.971251Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(num_most_common,\n",
    "                remove_stop_words=False,\n",
    "                lemmatize=False,\n",
    "                stem=False,\n",
    "                tf_idf=False):\n",
    "    all_words = [word.lower() for word in movie_reviews.words()]\n",
    "    if remove_stop_words:\n",
    "        all_words = [\n",
    "            word for word in all_words if word not in stops and word.isalnum()\n",
    "        ]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if lemmatize:\n",
    "        all_words = [lemmatizer.lemmatize(word, pos=\"a\") for word in all_words]\n",
    "    ps = PorterStemmer()\n",
    "    if stem:\n",
    "        all_words = [ps.stem(word) for word in all_words]\n",
    "    all_words_freq = nltk.FreqDist(all_words)\n",
    "    vocabulary = all_words_freq.most_common(50 + num_most_common)\n",
    "    vocabulary = vocabulary[50:]\n",
    "    dataset = [[], []]\n",
    "    if tf_idf:\n",
    "        collection = nltk.TextCollection(\n",
    "            [documents[i][0] for i in range(0, num_documents)])\n",
    "    for document, category in documents:\n",
    "        if lemmatize:\n",
    "            document = [\n",
    "                lemmatizer.lemmatize(word, pos=\"a\") for word in document\n",
    "            ]\n",
    "        if stem:\n",
    "            document = [ps.stem(word) for word in document]\n",
    "        if not tf_idf:\n",
    "            document_freq = nltk.FreqDist(document)\n",
    "            features = [\n",
    "                document_freq[word] / freq for word, freq in vocabulary\n",
    "            ]\n",
    "        else:\n",
    "            features = [\n",
    "                collection.tf_idf(word, document) for word, _ in vocabulary\n",
    "            ]\n",
    "        dataset[0].append(features)\n",
    "        dataset[1].append(category)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поделба на податочното множество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T20:52:58.983330Z",
     "start_time": "2018-12-06T20:52:58.979326Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    train_set = [dataset[0][:1500], dataset[1][:1500]]\n",
    "    test_set = [dataset[0][1500:], dataset[1][1500:]]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тренирање и тестирање на класификаторите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T20:53:00.119815Z",
     "start_time": "2018-12-06T20:53:00.098696Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:08:22.329280Z",
     "start_time": "2018-12-07T01:08:22.316255Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_test_models(train_set, test_set):\n",
    "    scores = {}\n",
    "    for i in range(0, 3):\n",
    "        if i == 0:\n",
    "            start_time = time.perf_counter()\n",
    "            knn = KNeighborsClassifier(\n",
    "                n_neighbors=20,\n",
    "                weights=\"distance\",\n",
    "                algorithm=\"auto\",\n",
    "                metric=\"cosine\",\n",
    "                n_jobs=-1)\n",
    "            knn.fit(train_set[0], train_set[1])\n",
    "            scores.setdefault(\"KNN\", [])\n",
    "            scores[\"KNN\"].append((knn.score(test_set[0], test_set[1]),\n",
    "                                  time.perf_counter() - start_time))\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            svm = SVC(\n",
    "                C=1e8,\n",
    "                kernel='rbf',\n",
    "                gamma=\"scale\",\n",
    "                decision_function_shape=\"ovr\",\n",
    "                class_weight=\"balanced\")\n",
    "            svm.fit(train_set[0], train_set[1])\n",
    "            scores.setdefault(\"SVM\", [])\n",
    "            scores[\"SVM\"].append((svm.score(test_set[0], test_set[1]),\n",
    "                                  time.perf_counter() - start_time))\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            logistic_regression = LogisticRegression(\n",
    "                C=1e8,\n",
    "                solver='newton-cg',\n",
    "                max_iter=1000,\n",
    "                class_weight=\"balanced\",\n",
    "                n_jobs=-1)\n",
    "            logistic_regression.fit(train_set[0], train_set[1])\n",
    "            scores.setdefault(\"Logistic Regression\", [])\n",
    "            scores[\"Logistic Regression\"].append((logistic_regression.score(\n",
    "                test_set[0], test_set[1]), time.perf_counter() - start_time))\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=1000,\n",
    "            criterion=\"entropy\",\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1)\n",
    "        random_forest.fit(train_set[0], train_set[1])\n",
    "        scores.setdefault(\"Random Forest\", [])\n",
    "        scores[\"Random Forest\"].append((random_forest.score(\n",
    "            test_set[0], test_set[1]), time.perf_counter() - start_time))\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        neural_network = MLPClassifier(\n",
    "            hidden_layer_sizes=(50, ),\n",
    "            activation=\"tanh\",\n",
    "            solver=\"adam\",\n",
    "            learning_rate=\"adaptive\",\n",
    "            learning_rate_init=0.001,\n",
    "            max_iter=1000,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1)\n",
    "        neural_network.fit(train_set[0], train_set[1])\n",
    "        scores.setdefault(\"Neural Network\", [])\n",
    "        scores[\"Neural Network\"].append((neural_network.score(\n",
    "            test_set[0], test_set[1]), time.perf_counter() - start_time))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:13:21.160402Z",
     "start_time": "2018-12-07T01:08:45.274246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': [(0.694, 2.075459099998625)], 'SVM': [(0.774, 1.4531935999984853)], 'Logistic Regression': [(0.766, 3.4254920999992464)], 'Random Forest': [(0.806, 1.394825800000035), (0.816, 1.404930400000012), (0.822, 1.2846406000007846)], 'Neural Network': [(0.794, 1.5728601999999228), (0.732, 0.6924945000009757), (0.77, 0.7208383999986836)]}\n",
      "{'KNN': [(0.69, 0.13019170000006852)], 'SVM': [(0.782, 1.7819963000001735)], 'Logistic Regression': [(0.8, 3.041087499999776)], 'Random Forest': [(0.844, 1.756692099999782), (0.852, 1.7504798999998457), (0.854, 1.5270831999987422)], 'Neural Network': [(0.804, 0.6036624999997002), (0.792, 0.8389827999999397), (0.806, 1.17682109999987)]}\n",
      "{'KNN': [(0.708, 0.21936710000045423)], 'SVM': [(0.812, 2.8707878999994136)], 'Logistic Regression': [(0.814, 1.562001500000406)], 'Random Forest': [(0.852, 1.6245164000010845), (0.832, 1.630780199999208), (0.84, 1.5195043000003352)], 'Neural Network': [(0.802, 0.9722615999999107), (0.812, 1.4308983000009903), (0.814, 0.9876260000000912)]}\n",
      "{'KNN': [(0.674, 0.2926601999988634)], 'SVM': [(0.75, 5.320608599999105)], 'Logistic Regression': [(0.768, 2.7709469000001263)], 'Random Forest': [(0.852, 1.9441674999998213), (0.86, 2.0912404999999126), (0.852, 1.8505103999996209)], 'Neural Network': [(0.81, 0.8369483999995282), (0.82, 2.2433357999998407), (0.804, 1.2252877000009903)]}\n",
      "{'KNN': [(0.708, 0.26551679999829503)], 'SVM': [(0.766, 7.94216459999916)], 'Logistic Regression': [(0.77, 3.0636250999996264)], 'Random Forest': [(0.848, 2.175521099999969), (0.854, 1.9608329000002414), (0.848, 1.9727148000001762)], 'Neural Network': [(0.804, 2.250704099998984), (0.812, 2.4502552000012656), (0.75, 0.8010386000005383)]}\n",
      "{'KNN': [(0.69, 0.31227360000048066)], 'SVM': [(0.78, 8.370079199999964)], 'Logistic Regression': [(0.784, 4.834182399999918)], 'Random Forest': [(0.862, 2.318385099999432), (0.85, 2.2114250999984506), (0.862, 2.393262699999468)], 'Neural Network': [(0.77, 1.7557128999997076), (0.8, 5.454440999999861), (0.8, 3.7215027999991435)]}\n",
      "{'KNN': [(0.708, 0.3554518000000826)], 'SVM': [(0.774, 9.702552499999001)], 'Logistic Regression': [(0.774, 6.307449699999779)], 'Random Forest': [(0.842, 2.725335500001165), (0.87, 2.5011888000008184), (0.872, 3.048427100000481)], 'Neural Network': [(0.81, 3.9840158999995765), (0.794, 2.197586500000398), (0.798, 1.954890199998772)]}\n",
      "{'KNN': [(0.67, 0.4010394999986602)], 'SVM': [(0.78, 11.434857199999897)], 'Logistic Regression': [(0.796, 5.029374399999142)], 'Random Forest': [(0.866, 3.1104419999992388), (0.862, 3.080822399999306), (0.862, 2.836012400000982)], 'Neural Network': [(0.796, 2.610159500000009), (0.784, 1.8974134999989474), (0.796, 1.8150698999997985)]}\n",
      "{'KNN': [(0.648, 0.4465653999995993)], 'SVM': [(0.746, 13.773308899999392)], 'Logistic Regression': [(0.774, 5.441927600000781)], 'Random Forest': [(0.886, 3.5117558000001736), (0.872, 3.790928200000053), (0.87, 3.0884525000001304)], 'Neural Network': [(0.798, 2.209614400000646), (0.816, 6.413761899999372), (0.806, 3.072825700000976)]}\n"
     ]
    }
   ],
   "source": [
    "for num_most_common in range(500, 5000, 500):\n",
    "    dataset = get_dataset(num_most_common)\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    scores = train_and_test_models(train_set, test_set)\n",
    "    for classifier, accuracies in scores.items():\n",
    "        if len(accuracies) == 1:\n",
    "            results.append([\n",
    "                1, num_most_common, accuracies[0][1], classifier,\n",
    "                accuracies[0][0], \"\"\n",
    "            ])\n",
    "            continue\n",
    "        for i in range(0, 3):\n",
    "            results.append([\n",
    "                i + 1, num_most_common, accuracies[i][1], classifier,\n",
    "                accuracies[i][0], \"\"\n",
    "            ])\n",
    "        averages = np.average(accuracies, axis=0)\n",
    "        results.append([\n",
    "            \"Average\", num_most_common, averages[1], classifier, averages[0],\n",
    "            \"\"\n",
    "        ])\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:18:52.623415Z",
     "start_time": "2018-12-07T01:13:58.838384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': [(0.692, 0.08500749999984691)], 'SVM': [(0.734, 2.6644753999989916)], 'Logistic Regression': [(0.722, 6.687809799999741)], 'Random Forest': [(0.78, 1.3858206999993854), (0.786, 1.6880492999989656), (0.786, 1.8798473000006197)], 'Neural Network': [(0.508, 0.3096253000003344), (0.768, 0.8515210999994451), (0.768, 0.955945799998517)]}\n",
      "{'KNN': [(0.71, 0.14627590000054624)], 'SVM': [(0.75, 1.8547393000008014)], 'Logistic Regression': [(0.768, 1.7247739000013098)], 'Random Forest': [(0.824, 1.7261027000004106), (0.826, 1.5074475999990682), (0.822, 1.6424719999995432)], 'Neural Network': [(0.774, 0.9001575999991474), (0.812, 1.8618181000001641), (0.794, 0.9587705000012647)]}\n",
      "{'KNN': [(0.72, 0.2259267000008549)], 'SVM': [(0.78, 3.6952806999997847)], 'Logistic Regression': [(0.806, 1.9059530000013183)], 'Random Forest': [(0.822, 1.8438833999989583), (0.838, 1.73754640000152), (0.836, 1.7313615000002756)], 'Neural Network': [(0.808, 1.8134953999997379), (0.814, 1.183265699999538), (0.816, 2.43597520000003)]}\n",
      "{'KNN': [(0.684, 0.2553840000000491)], 'SVM': [(0.75, 5.177695300000778)], 'Logistic Regression': [(0.754, 2.6683014999998704)], 'Random Forest': [(0.838, 2.118696000001364), (0.844, 1.9784534000009444), (0.83, 1.9490981999988435)], 'Neural Network': [(0.788, 0.8677559000007022), (0.782, 1.6949637999987317), (0.778, 1.534010399998806)]}\n",
      "{'KNN': [(0.68, 0.28157129999999597)], 'SVM': [(0.754, 6.701270499999737)], 'Logistic Regression': [(0.76, 3.180735800000548)], 'Random Forest': [(0.838, 2.320064399998955), (0.838, 2.283374000000549), (0.842, 2.2925328000001173)], 'Neural Network': [(0.782, 0.8999833999987459), (0.764, 1.2245488999997178), (0.784, 1.3062882000012905)]}\n",
      "{'KNN': [(0.698, 0.35979670000051556)], 'SVM': [(0.776, 8.345613199999207)], 'Logistic Regression': [(0.782, 5.481641000000309)], 'Random Forest': [(0.85, 2.729192699998748), (0.852, 2.792652199999793), (0.86, 2.8169850000012957)], 'Neural Network': [(0.778, 2.253411600000618), (0.794, 1.6025126000004093), (0.802, 1.8676087999992887)]}\n",
      "{'KNN': [(0.706, 0.35642319999897154)], 'SVM': [(0.756, 9.817185300000347)], 'Logistic Regression': [(0.766, 6.021131500001502)], 'Random Forest': [(0.844, 3.1941185999985464), (0.852, 3.0491115000004356), (0.85, 3.0306308999988687)], 'Neural Network': [(0.806, 6.348849900001369), (0.81, 1.992468500000541), (0.8, 3.0238819000005606)]}\n",
      "{'KNN': [(0.656, 0.3891957000014372)], 'SVM': [(0.768, 11.636108000000604)], 'Logistic Regression': [(0.768, 5.777034099999582)], 'Random Forest': [(0.862, 3.4996424000000843), (0.852, 3.1950243999999657), (0.856, 3.2494504999995115)], 'Neural Network': [(0.8, 5.539329799999905), (0.816, 3.89127820000067), (0.818, 5.215159699999276)]}\n",
      "{'KNN': [(0.658, 0.4179492000002938)], 'SVM': [(0.746, 13.048351800000091)], 'Logistic Regression': [(0.77, 13.912845400000151)], 'Random Forest': [(0.858, 3.6177589999988413), (0.854, 3.7162458000002516), (0.856, 4.134283999999752)], 'Neural Network': [(0.778, 2.705379699998957), (0.782, 2.8967756000001827), (0.786, 2.426025799999479)]}\n"
     ]
    }
   ],
   "source": [
    "for num_most_common in range(500, 5000, 500):\n",
    "    dataset = get_dataset(num_most_common, remove_stop_words=True)\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    scores = train_and_test_models(train_set, test_set)\n",
    "    for classifier, accuracies in scores.items():\n",
    "        if len(accuracies) == 1:\n",
    "            results.append([\n",
    "                1, num_most_common, accuracies[0][1], classifier,\n",
    "                accuracies[0][0], \"Excluded stopwords and non-word characters\"\n",
    "            ])\n",
    "            continue\n",
    "        for i in range(0, 3):\n",
    "            results.append([\n",
    "                i + 1, num_most_common, accuracies[i][1], classifier,\n",
    "                accuracies[i][0], \"Excluded stopwords and non-word characters\"\n",
    "            ])\n",
    "        averages = np.average(accuracies, axis=0)\n",
    "        results.append([\n",
    "            \"Average\", num_most_common, averages[1], classifier, averages[0],\n",
    "            \"Excluded stopwords and non-word characters\"\n",
    "        ])\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:25:06.957552Z",
     "start_time": "2018-12-07T01:19:27.703306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': [(0.7, 0.09187419999943813)], 'SVM': [(0.752, 1.4873363000006066)], 'Logistic Regression': [(0.75, 2.3479930000012246)], 'Random Forest': [(0.808, 1.5012074000005668), (0.816, 1.4315474999984872), (0.822, 1.3892728999999235)], 'Neural Network': [(0.786, 1.7144014999994397), (0.782, 0.789157699999123), (0.508, 0.3011201000008441)]}\n",
      "{'KNN': [(0.696, 0.14948010000080103)], 'SVM': [(0.79, 1.7595232999992731)], 'Logistic Regression': [(0.792, 1.6268426999995427)], 'Random Forest': [(0.854, 1.5165271000005305), (0.846, 1.7172169000004942), (0.848, 1.615939100000105)], 'Neural Network': [(0.82, 2.1804271999990306), (0.79, 0.9276778000003105), (0.808, 0.7156794000002265)]}\n",
      "{'KNN': [(0.73, 0.2164811999991798)], 'SVM': [(0.788, 4.574167599999782)], 'Logistic Regression': [(0.8, 1.747228800000812)], 'Random Forest': [(0.854, 1.635209500000201), (0.854, 1.7306091999998898), (0.846, 1.6204329000011057)], 'Neural Network': [(0.8, 0.6400164999995468), (0.812, 0.8336199999994278), (0.81, 1.3636043000005884)]}\n",
      "{'KNN': [(0.672, 0.2467413000013039)], 'SVM': [(0.738, 5.194371699999465)], 'Logistic Regression': [(0.768, 2.389039400000911)], 'Random Forest': [(0.85, 1.8594069999999192), (0.86, 2.050631100000828), (0.862, 1.8642142999997304)], 'Neural Network': [(0.818, 2.2270117000007303), (0.782, 0.7134195999988151), (0.812, 1.891713000000891)]}\n",
      "{'KNN': [(0.716, 0.2898865000006481)], 'SVM': [(0.758, 8.458124200000384)], 'Logistic Regression': [(0.774, 3.7856857999995555)], 'Random Forest': [(0.86, 2.525243000000046), (0.856, 2.30787699999928), (0.862, 2.319019999999)], 'Neural Network': [(0.796, 1.1799489000004542), (0.788, 1.2341310000010708), (0.782, 0.8951673000010487)]}\n",
      "{'KNN': [(0.686, 0.3053945999999996)], 'SVM': [(0.762, 8.167695100000856)], 'Logistic Regression': [(0.782, 4.736000300001251)], 'Random Forest': [(0.856, 2.2853269999995973), (0.862, 2.286867799999527), (0.878, 2.29998119999982)], 'Neural Network': [(0.806, 4.890678599998864), (0.778, 2.7590272999987064), (0.778, 3.813999599999079)]}\n",
      "{'KNN': [(0.71, 0.42821700000058627)], 'SVM': [(0.772, 10.068067499998506)], 'Logistic Regression': [(0.768, 5.815839200000482)], 'Random Forest': [(0.864, 2.851902500000506), (0.87, 2.750096699999631), (0.868, 2.7638831000003847)], 'Neural Network': [(0.794, 2.3887905000010505), (0.76, 1.8906477999989875), (0.776, 2.599700299999313)]}\n",
      "{'KNN': [(0.678, 0.3993985999986762)], 'SVM': [(0.776, 11.534587199999805)], 'Logistic Regression': [(0.788, 5.076386200000343)], 'Random Forest': [(0.868, 2.9632423000002746), (0.87, 3.1246296999997867), (0.858, 3.0673413000004075)], 'Neural Network': [(0.812, 3.0222093000011228), (0.83, 5.465869900001053), (0.812, 4.131451199998992)]}\n",
      "{'KNN': [(0.65, 0.43231070000001637)], 'SVM': [(0.762, 13.410029100001339)], 'Logistic Regression': [(0.776, 5.468583099998796)], 'Random Forest': [(0.88, 3.4026398999994854), (0.878, 3.4863304999998945), (0.882, 3.397019399999408)], 'Neural Network': [(0.784, 3.3322498999987147), (0.802, 2.3001331000014034), (0.792, 2.697036899999148)]}\n"
     ]
    }
   ],
   "source": [
    "for num_most_common in range(500, 5000, 500):\n",
    "    dataset = get_dataset(num_most_common, lemmatize=True)\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    scores = train_and_test_models(train_set, test_set)\n",
    "    for classifier, accuracies in scores.items():\n",
    "        if len(accuracies) == 1:\n",
    "            results.append([\n",
    "                1, num_most_common, accuracies[0][1], classifier,\n",
    "                accuracies[0][0], \"Using lemmatization\"\n",
    "            ])\n",
    "            continue\n",
    "        for i in range(0, 3):\n",
    "            results.append([\n",
    "                i + 1, num_most_common, accuracies[i][1], classifier,\n",
    "                accuracies[i][0], \"Using lemmatization\"\n",
    "            ])\n",
    "        averages = np.average(accuracies, axis=0)\n",
    "        results.append([\n",
    "            \"Average\", num_most_common, averages[1], classifier, averages[0],\n",
    "            \"Using lemmatization\"\n",
    "        ])\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T00:25:37.176593Z",
     "start_time": "2018-12-05T00:25:37.172591Z"
    }
   },
   "source": [
    "### Задача 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:35:45.240219Z",
     "start_time": "2018-12-07T01:25:43.991038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': [(0.72, 0.08786519999921438)], 'SVM': [(0.744, 1.5435631000000285)], 'Logistic Regression': [(0.74, 3.8194909999983793)], 'Random Forest': [(0.816, 1.3886719000001904), (0.808, 1.390149200000451), (0.808, 1.3917762000019138)], 'Neural Network': [(0.758, 1.245292800002062), (0.76, 0.7222456999988935), (0.764, 0.4340044999989914)]}\n",
      "{'KNN': [(0.72, 0.23687839999911375)], 'SVM': [(0.76, 1.7139036999978998)], 'Logistic Regression': [(0.768, 1.626897399997688)], 'Random Forest': [(0.852, 1.409171699997387), (0.848, 1.5076493000015034), (0.848, 1.4994922000005317)], 'Neural Network': [(0.782, 1.1971032000001287), (0.782, 0.7757199000006949), (0.79, 1.1498155999979645)]}\n",
      "{'KNN': [(0.684, 0.217906399997446)], 'SVM': [(0.772, 3.612351199997647)], 'Logistic Regression': [(0.764, 1.9863560999983747)], 'Random Forest': [(0.846, 1.6230327999983274), (0.838, 1.8334398999977566), (0.84, 1.859666099997412)], 'Neural Network': [(0.802, 2.94716740000149), (0.764, 0.7060448999982327), (0.8, 2.317811199998687)]}\n",
      "{'KNN': [(0.664, 0.2688646000024164)], 'SVM': [(0.728, 5.187102100000629)], 'Logistic Regression': [(0.746, 2.3739599000000453)], 'Random Forest': [(0.856, 1.8593522000010125), (0.834, 1.8526478000021598), (0.852, 1.8412423000008857)], 'Neural Network': [(0.776, 1.2982749000002514), (0.776, 0.794856600001367), (0.81, 1.9849556000008306)]}\n",
      "{'KNN': [(0.68, 0.2794733999980963)], 'SVM': [(0.728, 6.822122500001569)], 'Logistic Regression': [(0.744, 3.0348403999996663)], 'Random Forest': [(0.846, 2.0703057999999146), (0.84, 2.2711795999966853), (0.848, 2.738646999998309)], 'Neural Network': [(0.78, 1.3957016999993357), (0.772, 1.2001840000011725), (0.764, 0.8700510999988182)]}\n",
      "{'KNN': [(0.66, 0.4055898000005982)], 'SVM': [(0.724, 9.671846700002789)], 'Logistic Regression': [(0.742, 5.525860800000373)], 'Random Forest': [(0.866, 2.2223258999983955), (0.84, 2.282314799998858), (0.844, 2.389958999996452)], 'Neural Network': [(0.748, 1.636296599997877), (0.786, 3.3243258999973477), (0.8, 4.633466999999655)]}\n",
      "{'KNN': [(0.66, 0.35153820000050473)], 'SVM': [(0.778, 9.541815799999313)], 'Logistic Regression': [(0.782, 4.5453866999996535)], 'Random Forest': [(0.848, 2.6272602999997616), (0.854, 2.7867138999972667), (0.844, 2.7073009999985516)], 'Neural Network': [(0.782, 5.0810701999980665), (0.776, 1.6875342999992426), (0.794, 4.662900300001638)]}\n",
      "{'KNN': [(0.67, 0.39363740000044345)], 'SVM': [(0.746, 11.911492299997917)], 'Logistic Regression': [(0.756, 5.796317400003318)], 'Random Forest': [(0.844, 2.8610135999988415), (0.846, 2.9512740000027406), (0.838, 2.8683668000012403)], 'Neural Network': [(0.808, 6.779894300001615), (0.77, 2.028787999999622), (0.796, 1.9442238000010548)]}\n",
      "{'KNN': [(0.682, 0.4290402000005997)], 'SVM': [(0.726, 12.793288000000757)], 'Logistic Regression': [(0.748, 13.504413900001964)], 'Random Forest': [(0.858, 3.1047623999984353), (0.856, 3.054368000000977), (0.848, 3.0811620999993465)], 'Neural Network': [(0.78, 7.209790000000794), (0.798, 2.3566356000010273), (0.784, 2.7249392999983684)]}\n"
     ]
    }
   ],
   "source": [
    "for num_most_common in range(500, 5000, 500):\n",
    "    dataset = get_dataset(num_most_common, stem=True)\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    scores = train_and_test_models(train_set, test_set)\n",
    "    for classifier, accuracies in scores.items():\n",
    "        if len(accuracies) == 1:\n",
    "            results.append([\n",
    "                1, num_most_common, accuracies[0][1], classifier,\n",
    "                accuracies[0][0], \"Using stemmed words\"\n",
    "            ])\n",
    "            continue\n",
    "        for i in range(0, 3):\n",
    "            results.append([\n",
    "                i + 1, num_most_common, accuracies[i][1], classifier,\n",
    "                accuracies[i][0], \"Using stemmed words\"\n",
    "            ])\n",
    "        averages = np.average(accuracies, axis=0)\n",
    "        results.append([\n",
    "            \"Average\", num_most_common, averages[1], classifier, averages[0],\n",
    "            \"Using stemmed words\"\n",
    "        ])\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:46:21.479191Z",
     "start_time": "2018-12-07T01:36:23.600624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': [(0.704, 0.08505049999803305)], 'SVM': [(0.73, 2.1081582000006165)], 'Logistic Regression': [(0.722, 5.38599659999818)], 'Random Forest': [(0.794, 1.407507900003111), (0.786, 1.3820592999982182), (0.792, 1.383310100001836)], 'Neural Network': [(0.774, 1.2517707999977574), (0.734, 0.621055299998261), (0.728, 0.5421199000011256)]}\n",
      "{'KNN': [(0.698, 0.13400309999997262)], 'SVM': [(0.738, 1.7878171000011207)], 'Logistic Regression': [(0.756, 1.524373900003411)], 'Random Forest': [(0.828, 1.3945811000012327), (0.808, 1.4977651000008336), (0.82, 1.499621599999955)], 'Neural Network': [(0.772, 1.9092453000012028), (0.736, 1.3199001000029966), (0.736, 0.9966940999984217)]}\n",
      "{'KNN': [(0.652, 0.20761880000281963)], 'SVM': [(0.762, 3.561082099997293)], 'Logistic Regression': [(0.768, 2.2367432999999437)], 'Random Forest': [(0.824, 1.627124200000253), (0.83, 1.7439293999996153), (0.828, 1.7325737000028312)], 'Neural Network': [(0.79, 3.34192880000046), (0.774, 0.8196637000000919), (0.782, 1.4711486999985937)]}\n",
      "{'KNN': [(0.648, 0.24302379999789991)], 'SVM': [(0.706, 5.079374000000826)], 'Logistic Regression': [(0.712, 2.6382071000007272)], 'Random Forest': [(0.832, 1.8762518000003183), (0.828, 2.164888700001029), (0.828, 2.067432399999234)], 'Neural Network': [(0.764, 1.5215213000010408), (0.772, 3.7074477999994997), (0.746, 0.7974307000004046)]}\n",
      "{'KNN': [(0.64, 0.30161800000132644)], 'SVM': [(0.708, 6.843397900000127)], 'Logistic Regression': [(0.72, 4.103035699998145)], 'Random Forest': [(0.822, 2.1819586000019626), (0.818, 2.620549100000062), (0.818, 2.2136643000012555)], 'Neural Network': [(0.77, 1.3073121999987052), (0.774, 1.0604342000006), (0.782, 2.4274880000011763)]}\n",
      "{'KNN': [(0.632, 0.30639309999969555)], 'SVM': [(0.71, 8.255597899998975)], 'Logistic Regression': [(0.742, 4.859885099998792)], 'Random Forest': [(0.83, 2.3919439999990573), (0.832, 2.385552600000665), (0.84, 2.3819416999976966)], 'Neural Network': [(0.8, 4.250031799998396), (0.804, 5.433404799998243), (0.764, 1.4447161999996752)]}\n",
      "{'KNN': [(0.634, 0.3521823000010045)], 'SVM': [(0.756, 9.911304900000687)], 'Logistic Regression': [(0.784, 5.456588700002612)], 'Random Forest': [(0.834, 2.723057200000767), (0.838, 2.9423276000015903), (0.84, 2.738962500003254)], 'Neural Network': [(0.788, 1.9316203000016685), (0.724, 1.6595039999992878), (0.798, 2.3672540000006848)]}\n",
      "{'KNN': [(0.626, 0.38244489999851794)], 'SVM': [(0.728, 11.261150900001667)], 'Logistic Regression': [(0.75, 6.563878599998134)], 'Random Forest': [(0.82, 2.957094199999119), (0.83, 3.0523192000000563), (0.828, 2.9676866000008886)], 'Neural Network': [(0.778, 2.9811138000004576), (0.806, 5.923329900000681), (0.782, 2.5670930000014778)]}\n",
      "{'KNN': [(0.622, 0.5263713999993342)], 'SVM': [(0.722, 13.15651440000147)], 'Logistic Regression': [(0.748, 6.064229700001306)], 'Random Forest': [(0.83, 3.5189925999984553), (0.84, 3.2978380999993533), (0.838, 3.4976862000003166)], 'Neural Network': [(0.79, 2.1591909999988275), (0.8, 6.194335800002591), (0.774, 2.688934000001609)]}\n"
     ]
    }
   ],
   "source": [
    "for num_most_common in range(500, 5000, 500):\n",
    "    dataset = get_dataset(\n",
    "        num_most_common, remove_stop_words=True, lemmatize=True, stem=True)\n",
    "    train_set, test_set = split_dataset(dataset)\n",
    "    scores = train_and_test_models(train_set, test_set)\n",
    "    for classifier, accuracies in scores.items():\n",
    "        if len(accuracies) == 1:\n",
    "            results.append([\n",
    "                1, num_most_common, accuracies[0][1], classifier,\n",
    "                accuracies[0][0],\n",
    "                \"Excluded stopwords and non-words+lemmatization+stemming\"\n",
    "            ])\n",
    "            continue\n",
    "        for i in range(0, 3):\n",
    "            results.append([\n",
    "                i + 1, num_most_common, accuracies[i][1], classifier,\n",
    "                accuracies[i][0],\n",
    "                \"Excluded stopwords and non-words+lemmatization+stemming\"\n",
    "            ])\n",
    "        averages = np.average(accuracies, axis=0)\n",
    "        results.append([\n",
    "            \"Average\", num_most_common, averages[1], classifier, averages[0],\n",
    "            \"Excluded stopwords and non-words+lemmatization+stemming\"\n",
    "        ])\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:47:10.972827Z",
     "start_time": "2018-12-07T01:46:56.568529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_dataset = get_dataset(2500, lemmatize=True)\n",
    "train_set, test_set = split_dataset(bag_of_words_dataset)\n",
    "best_model = random_forest = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    criterion=\"entropy\",\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1)\n",
    "best_model.fit(train_set[0], train_set[1])\n",
    "best_model.score(test_set[0], test_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T02:48:25.441371Z",
     "start_time": "2018-12-07T01:53:05.675705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_dataset = get_dataset(2500, lemmatize=True, tf_idf=True)\n",
    "train_set, test_set = split_dataset(tf_idf_dataset)\n",
    "best_model.fit(train_set[0], train_set[1])\n",
    "best_model.score(test_set[0], test_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:47:38.072368Z",
     "start_time": "2018-12-07T01:47:37.957061Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Iteration\", \"Number of Features\", \"Duration (seconds)\", \"Classifier\",\n",
    "    \"Accuracy\", \"Comment\"\n",
    "]\n",
    "results_data_frame = pandas.DataFrame(data=results, columns=columns)\n",
    "writer = pandas.ExcelWriter('results.xlsx')\n",
    "results_data_frame.to_excel(writer, 'Results')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T01:48:04.675536Z",
     "start_time": "2018-12-07T01:48:04.656597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Number of Features</th>\n",
       "      <th>Duration (seconds)</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>2.075459</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.694000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.453194</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.774000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>3.425492</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.766000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.394826</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.806000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1.404930</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.816000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>1.284641</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.822000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Average</td>\n",
       "      <td>500</td>\n",
       "      <td>1.361466</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.814667</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.572860</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.794000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.692495</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.732000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>0.720838</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.770000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Average</td>\n",
       "      <td>500</td>\n",
       "      <td>0.995398</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.765333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.130192</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.690000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.781996</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.782000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>3.041087</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.800000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.756692</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.844000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.750480</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.852000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.527083</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.854000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Average</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.678085</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.850000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.603662</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.804000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.838983</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.792000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.176821</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.806000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Average</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.873155</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.800667</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.219367</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.708000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>2.870788</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.812000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.562002</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.814000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.624516</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.852000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.630780</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.832000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.519504</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.840000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Average</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.591600</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.841333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.972262</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.802000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>1</td>\n",
       "      <td>3500</td>\n",
       "      <td>2.723057</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2</td>\n",
       "      <td>3500</td>\n",
       "      <td>2.942328</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>3</td>\n",
       "      <td>3500</td>\n",
       "      <td>2.738963</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>Average</td>\n",
       "      <td>3500</td>\n",
       "      <td>2.801449</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.837333</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "      <td>3500</td>\n",
       "      <td>1.931620</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2</td>\n",
       "      <td>3500</td>\n",
       "      <td>1.659504</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>3</td>\n",
       "      <td>3500</td>\n",
       "      <td>2.367254</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Average</td>\n",
       "      <td>3500</td>\n",
       "      <td>1.986126</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.382445</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>11.261151</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>6.563879</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>2.957094</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2</td>\n",
       "      <td>4000</td>\n",
       "      <td>3.052319</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>2.967687</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Average</td>\n",
       "      <td>4000</td>\n",
       "      <td>2.992367</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>2.981114</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>2</td>\n",
       "      <td>4000</td>\n",
       "      <td>5.923330</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>3</td>\n",
       "      <td>4000</td>\n",
       "      <td>2.567093</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>Average</td>\n",
       "      <td>4000</td>\n",
       "      <td>3.823846</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.788667</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.526371</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>Excluded stopwords and non-words+lemmatization."
      ],
      "text/plain": [
       "    Iteration  Number of Features  Duration (seconds)           Classifier  \\\n",
       "0           1                 500            2.075459                  KNN   \n",
       "1           1                 500            1.453194                  SVM   \n",
       "2           1                 500            3.425492  Logistic Regression   \n",
       "3           1                 500            1.394826        Random Forest   \n",
       "4           2                 500            1.404930        Random Forest   \n",
       "5           3                 500            1.284641        Random Forest   \n",
       "6     Average                 500            1.361466        Random Forest   \n",
       "7           1                 500            1.572860       Neural Network   \n",
       "8           2                 500            0.692495       Neural Network   \n",
       "9           3                 500            0.720838       Neural Network   \n",
       "10    Average                 500            0.995398       Neural Network   \n",
       "11          1                1000            0.130192                  KNN   \n",
       "12          1                1000            1.781996                  SVM   \n",
       "13          1                1000            3.041087  Logistic Regression   \n",
       "14          1                1000            1.756692        Random Forest   \n",
       "15          2                1000            1.750480        Random Forest   \n",
       "16          3                1000            1.527083        Random Forest   \n",
       "17    Average                1000            1.678085        Random Forest   \n",
       "18          1                1000            0.603662       Neural Network   \n",
       "19          2                1000            0.838983       Neural Network   \n",
       "20          3                1000            1.176821       Neural Network   \n",
       "21    Average                1000            0.873155       Neural Network   \n",
       "22          1                1500            0.219367                  KNN   \n",
       "23          1                1500            2.870788                  SVM   \n",
       "24          1                1500            1.562002  Logistic Regression   \n",
       "25          1                1500            1.624516        Random Forest   \n",
       "26          2                1500            1.630780        Random Forest   \n",
       "27          3                1500            1.519504        Random Forest   \n",
       "28    Average                1500            1.591600        Random Forest   \n",
       "29          1                1500            0.972262       Neural Network   \n",
       "..        ...                 ...                 ...                  ...   \n",
       "465         1                3500            2.723057        Random Forest   \n",
       "466         2                3500            2.942328        Random Forest   \n",
       "467         3                3500            2.738963        Random Forest   \n",
       "468   Average                3500            2.801449        Random Forest   \n",
       "469         1                3500            1.931620       Neural Network   \n",
       "470         2                3500            1.659504       Neural Network   \n",
       "471         3                3500            2.367254       Neural Network   \n",
       "472   Average                3500            1.986126       Neural Network   \n",
       "473         1                4000            0.382445                  KNN   \n",
       "474         1                4000           11.261151                  SVM   \n",
       "475         1                4000            6.563879  Logistic Regression   \n",
       "476         1                4000            2.957094        Random Forest   \n",
       "477         2                4000            3.052319        Random Forest   \n",
       "478         3                4000            2.967687        Random Forest   \n",
       "479   Average                4000            2.992367        Random Forest   \n",
       "480         1                4000            2.981114       Neural Network   \n",
       "481         2                4000            5.923330       Neural Network   \n",
       "482         3                4000            2.567093       Neural Network   \n",
       "483   Average                4000            3.823846       Neural Network   \n",
       "484         1                4500            0.526371                  KNN   \n",
       "485         1                4500           13.156514                  SVM   \n",
       "486         1                4500            6.064230  Logistic Regression   \n",
       "487         1                4500            3.518993        Random Forest   \n",
       "488         2                4500            3.297838        Random Forest   \n",
       "489         3                4500            3.497686        Random Forest   \n",
       "490   Average                4500            3.438172        Random Forest   \n",
       "491         1                4500            2.159191       Neural Network   \n",
       "492         2                4500            6.194336       Neural Network   \n",
       "493         3                4500            2.688934       Neural Network   \n",
       "494   Average                4500            3.680820       Neural Network   \n",
       "\n",
       "     Accuracy                                            Comment  \n",
       "0    0.694000                                                     \n",
       "1    0.774000                                                     \n",
       "2    0.766000                                                     \n",
       "3    0.806000                                                     \n",
       "4    0.816000                                                     \n",
       "5    0.822000                                                     \n",
       "6    0.814667                                                     \n",
       "7    0.794000                                                     \n",
       "8    0.732000                                                     \n",
       "9    0.770000                                                     \n",
       "10   0.765333                                                     \n",
       "11   0.690000                                                     \n",
       "12   0.782000                                                     \n",
       "13   0.800000                                                     \n",
       "14   0.844000                                                     \n",
       "15   0.852000                                                     \n",
       "16   0.854000                                                     \n",
       "17   0.850000                                                     \n",
       "18   0.804000                                                     \n",
       "19   0.792000                                                     \n",
       "20   0.806000                                                     \n",
       "21   0.800667                                                     \n",
       "22   0.708000                                                     \n",
       "23   0.812000                                                     \n",
       "24   0.814000                                                     \n",
       "25   0.852000                                                     \n",
       "26   0.832000                                                     \n",
       "27   0.840000                                                     \n",
       "28   0.841333                                                     \n",
       "29   0.802000                                                     \n",
       "..        ...                                                ...  \n",
       "465  0.834000  Excluded stopwords and non-words+lemmatization...  \n",
       "466  0.838000  Excluded stopwords and non-words+lemmatization...  \n",
       "467  0.840000  Excluded stopwords and non-words+lemmatization...  \n",
       "468  0.837333  Excluded stopwords and non-words+lemmatization...  \n",
       "469  0.788000  Excluded stopwords and non-words+lemmatization...  \n",
       "470  0.724000  Excluded stopwords and non-words+lemmatization...  \n",
       "471  0.798000  Excluded stopwords and non-words+lemmatization...  \n",
       "472  0.770000  Excluded stopwords and non-words+lemmatization...  \n",
       "473  0.626000  Excluded stopwords and non-words+lemmatization...  \n",
       "474  0.728000  Excluded stopwords and non-words+lemmatization...  \n",
       "475  0.750000  Excluded stopwords and non-words+lemmatization...  \n",
       "476  0.820000  Excluded stopwords and non-words+lemmatization...  \n",
       "477  0.830000  Excluded stopwords and non-words+lemmatization...  \n",
       "478  0.828000  Excluded stopwords and non-words+lemmatization...  \n",
       "479  0.826000  Excluded stopwords and non-words+lemmatization...  \n",
       "480  0.778000  Excluded stopwords and non-words+lemmatization...  \n",
       "481  0.806000  Excluded stopwords and non-words+lemmatization...  \n",
       "482  0.782000  Excluded stopwords and non-words+lemmatization...  \n",
       "483  0.788667  Excluded stopwords and non-words+lemmatization...  \n",
       "484  0.622000  Excluded stopwords and non-words+lemmatization...  \n",
       "485  0.722000  Excluded stopwords and non-words+lemmatization...  \n",
       "486  0.748000  Excluded stopwords and non-words+lemmatization...  \n",
       "487  0.830000  Excluded stopwords and non-words+lemmatization...  \n",
       "488  0.840000  Excluded stopwords and non-words+lemmatization...  \n",
       "489  0.838000  Excluded stopwords and non-words+lemmatization...  \n",
       "490  0.836000  Excluded stopwords and non-words+lemmatization...  \n",
       "491  0.790000  Excluded stopwords and non-words+lemmatization...  \n",
       "492  0.800000  Excluded stopwords and non-words+lemmatization...  \n",
       "493  0.774000  Excluded stopwords and non-words+lemmatization...  \n",
       "494  0.788000  Excluded stopwords and non-words+lemmatization...  \n",
       "\n",
       "[495 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 12255 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_data_frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "665px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
